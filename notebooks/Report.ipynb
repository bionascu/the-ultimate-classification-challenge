{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme multi-class classification\n",
    "\n",
    "ID2223 Scalable Machine Learning and Deep Learning\n",
    "\n",
    "**Federico Baldassarre (fedbal@kth.se) and Beatrice Ionascu (bionascu@kth.se)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "\n",
    "# add the 'src' directory so we can import modules\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from utils.paths import data_raw_dir\n",
    "from utils.paths import data_processed_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "The goal of this project is to classify products from [Cdiscount](https://www.cdiscount.com/), Franceâ€™s largest non-food e-commerce company, based on the product images on the company's website. Being able to correctly predict the category of products is important in ensuring that new products are well classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data](../figures/data.png)\n",
    "\n",
    "The Cdiscount dataset consists of 15 million images at 180x180 resolution of almost 9\n",
    "million products. The training data consists of a list of 7,069,896 dictionaries, one per product. Each dictionary contains a product id, the category id of the product, and between\n",
    "1-4 images, stored in a list. In addition, each category id has a corresponding level1,\n",
    "level2, and level3 name, in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.read_pickle(os.path.join(data_processed_dir, 'categories.pickle'))\n",
    "display(HTML(categories.filter(like='category', axis=1).head().to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_id_prod_distrib.pickle'))\n",
    "print('There are {} products.'.format(product_distrib.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_id_img_distrib.pickle'))\n",
    "print('There are {} images.'.format(image_distrib.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_counts = pd.read_csv(os.path.join(data_raw_dir, 'category_names.csv'))\n",
    "print(categ_counts.nunique().to_frame('Category counts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5270 available categories are very unevenly distributed amongst the products. As seen below, there are many categories with just a few products and few categories with very many products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_id_prod_distrib.pickle'))\n",
    "cat_1_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_1_prod_distrib.pickle'))\n",
    "cat_2_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_2_prod_distrib.pickle'))\n",
    "cat_3_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_3_prod_distrib.pickle'))\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16,10))\n",
    "ax=ax.ravel()\n",
    "cat_1_distrib.hist(log=True, bins=50,ax=ax[0])\n",
    "ax[0].set_title('Category Level 1 ({})'.format(cat_1_distrib.size)); ax[0].set_xlabel('Number of products')\n",
    "cat_2_distrib.hist(log=True, bins=50,ax=ax[1])\n",
    "ax[1].set_title('Category Level 2 ({})'.format(cat_2_distrib.size)); ax[1].set_xlabel('Number of products')\n",
    "cat_3_distrib.hist(log=True, bins=50,ax=ax[2])\n",
    "ax[2].set_title('Category Level 3 ({})'.format(cat_3_distrib.size)); ax[2].set_xlabel('Number of products')\n",
    "cat_id_distrib.hist(log=True, bins=50,ax=ax[3])\n",
    "ax[3].set_title('Category id ({})'.format(cat_id_distrib.size)); ax[3].set_xlabel('Number of products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categories with the most and least products are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(categories.filter(like='category'), \n",
    "         cat_id_distrib.sort_values(ascending=False).iloc[np.r_[0:5, -5:0]].to_frame('counts'),\n",
    "         left_index=True, right_index=True).sort_values('counts', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "### Transfer Learning\n",
    "It's been proved, both in the literature and by practical applications, that an image classification task can benefit from transfer learning. They key concept is to reuse the architecture and the weights of another network to extract a representation of the images, which hopefully contains useful information for the classification task. Once an images have been embedded into a compressed feature space, a shallow network can be used to classify them into the final categories.\n",
    "\n",
    "The whole network can be sketched as:\n",
    "\n",
    "![xception+shallow network](../figures/architecture.png)\n",
    "\n",
    "During training the weights of the feature extraction network are frozen, reducing the weights to learn to only those of the shallow network at the end.\n",
    "\n",
    "The advantages of transfer learning are enabled by the availability of models trained on a very large and general image datasets and released to the public. In fact, training those models can take a long time (up to a few weeks) even in a distributed GPU environment, making it unfeasible to train them jointly with the shallow network at the end. Also, their depth would provide a challenge in terms of gradient propagation, especially at the beginning when the last layers try to classify an image on the base of an embedding that is almost random and therefore are unable to provide the previous layers with useful information on how to update their weights.\n",
    "\n",
    "We have chosen the recent [Xception model](https://arxiv.org/abs/1610.02357) for feature extraction over the more popular [VGG](https://arxiv.org/abs/1409.1556) or [Inception](https://arxiv.org/abs/1409.4842) models. The choice is motivated by the former being lighter and faster than the other two, achieving almost the same accuracy on the ImageNet benchmark ([comparison](https://keras.io/applications#documentation-for-individual-models))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification network architecture and negative sampling\n",
    "The architecture we used for the classification network is composed of three convolutional layers with droupout and a fully connected softmax layer. The purpose of the first 1x1 convolution is to reduce the depth of the embedding volume, before moving further down the network. The output of the final fully connected layer represents the probability distribution over the classes.\n",
    "\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape                Param #   \n",
    "=================================================================\n",
    "InputLayer                   (?, 6, 6, 2048)                 \n",
    "_________________________________________________________________\n",
    "Conv2D (1x1 kernel)          (?, 6, 6, 64)               131,136    \n",
    "_________________________________________________________________\n",
    "Dropout                      (?, 6, 6, 64)                   \n",
    "_________________________________________________________________\n",
    "Conv2D (4x4 kernel)          (?, 3, 3, 128)              131,200    \n",
    "_________________________________________________________________\n",
    "Dropout                      (?, 3, 3, 128)                  \n",
    "_________________________________________________________________\n",
    "Conv2D (3x3 kernel)          (?, 1, 1, 256)              295,168    \n",
    "_________________________________________________________________\n",
    "Dropout                      (?, 1, 1, 256)                  \n",
    "________________________________________________________________\n",
    "Flatten                      (?, 256)                        \n",
    "_________________________________________________________________\n",
    "Dense                        (?, 5270)                 1,354,390   \n",
    "=================================================================\n",
    "Trainable params: 1,911,894\n",
    "_________________________________________________________________\n",
    "```\n",
    "\n",
    "It is evident that in such architecture the main contribution to the number of trainable parameters comes from the fully connected layer. In fact, it is accountable for approx. 70% of the weights of the network. The reason for this is the high number of probability values to output (5270 versus the 1000 classes of ImageNet). \n",
    "\n",
    "Training a network with a large number of parameters turns out to be slow, but we can employ a trick from Natural Language Processing to improve the training speed. Instead of computing the cross entropy loss for all the 5270 output probabilities we can subsample the output vector to include the true label and part of the false labels and only compute the loss for these. In this way we backpropagate fewer gradients through the last layer and reduce the computational effort, effectively speding up training. Of course, the price to pay for subsampling the loss function is less informative gradients and lower final accuracy.\n",
    "\n",
    "TODO maybe image\n",
    "\n",
    "## Pre-processing\n",
    "### Label encoding\n",
    "\n",
    "## Feature Extraction\n",
    "### TfRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation graph\n",
    "![computation graph](../figures/computational_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- We trained the network using two different loss functions.\n",
    "- explain training and testing split\n",
    "- explain epoch / step/batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Results\n",
    "\n",
    "\n",
    "![results](../figures/metrics_v2.png)\n",
    "\n",
    "- classification accuracy and loss per image\n",
    "- can't test on the test data since we don't have access to it ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Future work\n",
    "\n",
    "- group images into products -- report results per product\n",
    "- test the influence of the negative sample size in sampled softmax\n",
    "- use heirarchy\n",
    "- use OCR for books\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
