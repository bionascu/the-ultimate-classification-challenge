{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme multi-class classification\n",
    "\n",
    "ID2223 Scalable Machine Learning and Deep Learning\n",
    "\n",
    "**Federico Baldassarre (fedbal@kth.se) and Beatrice Ionascu (bionascu@kth.se)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "\n",
    "# add the 'src' directory so we can import modules\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils.paths import data_raw_dir, data_processed_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "The goal of this project is to classify products from [Cdiscount](https://www.cdiscount.com/), Franceâ€™s largest non-food e-commerce company, based on the product images on the company's website. Being able to correctly predict the category of products is important in ensuring that new products are well classified.\n",
    "\n",
    "The main challenges are the size of the image dataset, the number of classes and the label imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cdiscount dataset consists of 15 million images at 180x180 resolution of almost 9\n",
    "million products. The training data consists of a list of 7,069,896 dictionaries, one per product. Each dictionary contains a product id, the category id of the product, and between\n",
    "1-4 images, stored in a list. In addition, each category id has a corresponding level1,\n",
    "level2, and level3 name, in French.\n",
    "\n",
    "![data](../figures/data_imgs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>category_id</th>\n",
       "      <th>category_level1</th>\n",
       "      <th>category_level2</th>\n",
       "      <th>category_level3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000021794</td>\n",
       "      <td>ABONNEMENT / SERVICES</td>\n",
       "      <td>CARTE PREPAYEE</td>\n",
       "      <td>CARTE PREPAYEE MULTIMEDIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000012764</td>\n",
       "      <td>AMENAGEMENT URBAIN - VOIRIE</td>\n",
       "      <td>AMENAGEMENT URBAIN</td>\n",
       "      <td>ABRI FUMEUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000012776</td>\n",
       "      <td>AMENAGEMENT URBAIN - VOIRIE</td>\n",
       "      <td>AMENAGEMENT URBAIN</td>\n",
       "      <td>ABRI VELO - ABRI MOTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000012768</td>\n",
       "      <td>AMENAGEMENT URBAIN - VOIRIE</td>\n",
       "      <td>AMENAGEMENT URBAIN</td>\n",
       "      <td>FONTAINE A EAU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000012755</td>\n",
       "      <td>AMENAGEMENT URBAIN - VOIRIE</td>\n",
       "      <td>SIGNALETIQUE</td>\n",
       "      <td>PANNEAU D'INFORMATION EXTERIEUR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = pd.read_pickle(os.path.join(data_processed_dir, 'categories.pickle'))\n",
    "display(HTML(categories.filter(like='category', axis=1).head().to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7069896 products\n",
      "There are 12371293 images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>category_id</th>\n",
       "      <td>5270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_level1</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_level2</th>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_level3</th>\n",
       "      <td>5263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Category counts\n",
       "category_id                 5270\n",
       "category_level1               49\n",
       "category_level2              483\n",
       "category_level3             5263"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_id_prod_distrib.pickle'))\n",
    "image_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_id_img_distrib.pickle'))\n",
    "categ_counts = pd.read_csv(os.path.join(data_raw_dir, 'category_names.csv'))\n",
    "\n",
    "print('There are {} products'.format(product_distrib.sum()))\n",
    "print('There are {} images'.format(image_distrib.sum()))\n",
    "categ_counts.nunique().to_frame('Category counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5270 available categories are very unevenly distributed amongst the products. As shown in the following histograms (one per category level), there are many categories with just a few products and few categories with a very high number of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_id_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_id_prod_distrib.pickle'))\n",
    "cat_1_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_1_prod_distrib.pickle'))\n",
    "cat_2_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_2_prod_distrib.pickle'))\n",
    "cat_3_distrib = pd.read_pickle(os.path.join(data_processed_dir, 'cat_3_prod_distrib.pickle'))\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16,10))\n",
    "ax=ax.ravel()\n",
    "cat_1_distrib.hist(log=True, bins=50,ax=ax[0])\n",
    "ax[0].set_title('Category Level 1 ({})'.format(cat_1_distrib.size))\n",
    "cat_2_distrib.hist(log=True, bins=50,ax=ax[1])\n",
    "ax[1].set_title('Category Level 2 ({})'.format(cat_2_distrib.size))\n",
    "cat_3_distrib.hist(log=True, bins=50,ax=ax[2])\n",
    "ax[2].set_title('Category Level 3 ({})'.format(cat_3_distrib.size))\n",
    "cat_id_distrib.hist(log=True, bins=50,ax=ax[3])\n",
    "ax[3].set_title('Category id ({})'.format(cat_id_distrib.size))\n",
    "for a in ax: a.set_xlabel('Number of products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categories with the most and least products are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(pd.merge(categories.filter(like='category'), \n",
    "         cat_id_distrib.sort_values(ascending=False).head().to_frame('product counts'),\n",
    "         left_index=True, right_index=True).sort_values('product counts', ascending=False).to_html(index=False)))\n",
    "\n",
    "display(HTML(pd.merge(categories.filter(like='category'), \n",
    "         cat_id_distrib.sort_values(ascending=False).tail().to_frame('product counts'),\n",
    "         left_index=True, right_index=True).sort_values('product counts', ascending=False).to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "### Transfer Learning\n",
    "It's been proved, both in the literature and by practical applications, that an image classification task can benefit from transfer learning. They key concept is to reuse the architecture and the weights of another network to extract a representation of the images, which hopefully contains useful information for the classification task. Once an images have been embedded into a compressed feature space, a shallow network can be used to classify them into the final categories.\n",
    "\n",
    "The skeleton of our network can be sketched as:\n",
    "\n",
    "![xception+shallow network](../figures/architecture.png)\n",
    "\n",
    "During training the weights of the feature extraction network are frozen, limiting the weights to train to only those of the shallow network at the end.\n",
    "\n",
    "The advantages of transfer learning are enabled by the availability of models trained on a very large and general image datasets and released to the public. In fact, training those models can take a long time (up to a few weeks) even in a distributed GPU environment, making it unfeasible to train them jointly with the shallow network at the end. Also, their depth would provide a challenge in terms of gradient propagation, especially at the beginning when the last layers try to classify an image on the base of an embedding that is almost random and therefore are unable to provide the previous layers with useful information on how to update their weights.\n",
    "\n",
    "We have chosen the recent [Xception model](https://arxiv.org/abs/1610.02357) for feature extraction over the more popular [VGG](https://arxiv.org/abs/1409.1556) or [Inception](https://arxiv.org/abs/1409.4842) models. The choice is motivated by the former being lighter and faster than the other two, achieving almost the same accuracy on the ImageNet benchmark ([comparison](https://keras.io/applications#documentation-for-individual-models))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification network architecture and negative sampling\n",
    "The architecture we used for the classification network is composed of three convolutional layers with dropout and a fully connected softmax layer. The purpose of the first 1x1 convolution is to reduce the depth of the embedding volume, before moving further down the network. The output of the final fully connected layer represents the probability distribution over the classes.\n",
    "\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape                Param #   \n",
    "=================================================================\n",
    "InputLayer                   (?, 6, 6, 2048)                 \n",
    "_________________________________________________________________\n",
    "Conv2D (1x1 kernel)          (?, 6, 6, 64)               131,136    \n",
    "_________________________________________________________________\n",
    "Dropout                      (?, 6, 6, 64)                   \n",
    "_________________________________________________________________\n",
    "Conv2D (4x4 kernel)          (?, 3, 3, 128)              131,200    \n",
    "_________________________________________________________________\n",
    "Dropout                      (?, 3, 3, 128)                  \n",
    "_________________________________________________________________\n",
    "Conv2D (3x3 kernel)          (?, 1, 1, 256)              295,168    \n",
    "_________________________________________________________________\n",
    "Dropout                      (?, 1, 1, 256)                  \n",
    "________________________________________________________________\n",
    "Flatten                      (?, 256)                        \n",
    "_________________________________________________________________\n",
    "Dense                        (?, 5270)                 1,354,390   \n",
    "=================================================================\n",
    "Trainable params: 1,911,894\n",
    "_________________________________________________________________\n",
    "```\n",
    "\n",
    "It is evident that in such architecture the main contribution to the number of trainable parameters comes from the fully connected layer. In fact, it is accountable for approx. 70% of the weights of the network. The reason for this is the high number of probability values to output (5270 versus the 1000 classes of ImageNet). \n",
    "\n",
    "Training a network with a large number of parameters turns out to be slow, but we can employ a trick from Natural Language Processing to improve the training speed. Instead of computing the cross entropy loss for all the 5270 output probabilities we can subsample the output vector to include the true label and part of the false labels and only compute the loss for these. In this way we backpropagate fewer gradients through the last layer and reduce the computational effort, effectively speding up training. Of course, the price to pay for subsampling the loss function is less informative gradients and lower final accuracy.\n",
    "\n",
    "TODO maybe image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Pre-processing and feature extraction\n",
    "- Label encoding\n",
    "- TfRecords\n",
    "\n",
    "X% of the 12 million images was used for training and Y% for testing. \n",
    "\n",
    "### Computation graph\n",
    "\n",
    "This is the computational graph built with the TensorFlow and Keras APIs. The weights of the classification network are at the center of the image, to their left is the training pipeline and to the right the testing pipeline. \n",
    "\n",
    "![computation graph](../figures/computational_graph.png)\n",
    "\n",
    "### Regular loss vs. negative sampled loss\n",
    "\n",
    "The network was trained with two different loss functions, regular softmax and sampled softmax in order to compare the accuracy and time performance of the two approaches. In both cases we use cross-entropy as loss, except that the sampled version only considers the true label and 1000 negative labels (~20% of all 5270)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The network was trained for Z epochs, using a batch size of XX. The accuracy and loss metrics are reported per optimization step (not per epoch).\n",
    "\n",
    "The classification accuracy and loss metrics obtained on the training and tests sets are shown here (X% smoothing). \n",
    "\n",
    "![results](../figures/metrics_v2.png)\n",
    "\n",
    "As expected,  \n",
    "????\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "\n",
    "##### Product-wise classification instead of image-wise\n",
    "  \n",
    "In the original setting of the problem, a set of images is given for each unknown product. Our network classifies each image singularly, ideally, a better classification algorithm would exploit all the images to output a single prediction for the product.\n",
    "  \n",
    "##### Evaluate the performances on the Kaggle test data\n",
    "\n",
    "Unfortunately, at the request of the sponsor, the data has been removed post-competition. For this reason, it is not possible to evaluate the performances of the network on the test set used for the competition.\n",
    "\n",
    "##### Hyperparameter tuning\n",
    "Having more computing power and time available, we would test the effect of these hyperparameters, both performance-wise and training time-wise:\n",
    "- negative sample size (speed/accuracy trade-off)\n",
    "- network architecture (number of layers, regularization)\n",
    "\n",
    "##### Others\n",
    "\n",
    "Other ways of improving the overall performances are:\n",
    "- Exploiting the hierarchy of the categories, e.g. first training a network to predict the (fewer) *category_1* labels, then extending it to the other categories\n",
    "- Extracting text from the images, which would prove extremely useful in the case of books, CDs and similar products, since they represent an important fraction of the dataset (about 10%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
